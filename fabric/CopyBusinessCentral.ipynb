{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae53e9bf-8787-4d07-b709-d896fd16cc5f",
   "metadata": {
    "editable": false,
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "source": [
    "## Business Central merge data notebook\n",
    "In this part the files in the delta folder will be merge with the Lakehouse table.\n",
    "- It iterates first on the folders to append to the existing table.\n",
    "- After that is will remove all duplicates by sorting the table. \n",
    "- At last it will remove all deleted records inside the table that are deleted in Business Central\n",
    "\n",
    "Please change the parameters in the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc5721-e317-4dc0-88ef-2c6bafb494da",
   "metadata": {
    "cellStatus": "{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.6812441Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:06.8530455Z\",\"execution_finish_time\":\"2023-08-15T09:15:07.1828235Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "# settings\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizewrite.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "spark.conf.set(\"spark.sql.delta.commitProtocol.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.analyzer.maxIterations\", \"999\")\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "\n",
    "# file paths\n",
    "folder_path_spark = 'Files/deltas/' # this is mostly the default\n",
    "folder_path_json = '/lakehouse/default/Files/' # this is mostly the default\n",
    "folder_path_reset = '/lakehouse/default/Files/reset/' # this is mostly the default\n",
    "folder_path = '/lakehouse/default/Files/deltas/' # this is mostly the default\n",
    "\n",
    "# parameters\n",
    "workspace = 'businessCentral' #can also be a GUID\n",
    "Lakehouse = 'businessCentral'; #can also be a GUID - if you do please add back-quotes around the GUID for example: '`GUID`'\n",
    "Remove_delta = True; #will remove the delta files if everything is processed\n",
    "Drop_table_if_mismatch = False; #option to drop the table if json file has different columns then in the table\n",
    "no_Partition = 258 #how many partition is used in the dataframe, a good starting point might be 2-4 partitions per CPU core in your Spark cluster\n",
    "DecimalFormat = 'float' #how to format the decimal numbers, can be 'float' or 'decimal(10,3)'. If you change this it will be a breaking change for the table\n",
    "DateTimeFormat = 'timestamp' #how to format the datetime, can be 'timestamp' or 'date'. If you change this it will be a breaking change for the table\n",
    "schema_name = \"\" #for if you are using a lakehouse based on a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def is_string_empty(s):\n",
    "    return not bool(s.strip())\n",
    "\n",
    "if Drop_table_if_mismatch:\n",
    "\n",
    "    def count_keys(obj):  \n",
    "        if isinstance(obj, dict):  \n",
    "            return len(obj) + sum(count_keys(v) for v in obj.values())  \n",
    "        if isinstance(obj, list):  \n",
    "            return sum(count_keys(v) for v in obj)  \n",
    "        return 0  \n",
    "\n",
    "    for filename in os.listdir(folder_path_json):\n",
    "        if \"manifest\" not in filename: # exclude the manifest files\n",
    "            if filename.endswith(\".cdm.json\"):\n",
    "                table_name = filename.replace(\"-\",\"\")\n",
    "                table_name = table_name.replace(\".cdm.json\",\"\")\n",
    "\n",
    "                if table_name in [t.name for t in spark.catalog.listTables()]:\n",
    "                    #count number of columns in excisting table\n",
    "                    if is_string_empty(schema_name):\n",
    "                        SQL_Query = \"SELECT * FROM \" + Lakehouse + \".\" + table_name + \" LIMIT 1\"\n",
    "                    else:\n",
    "                        SQL_Query = \"SELECT * FROM \" + Lakehouse + \".\" + schema_name +\".\" + table_name + \" LIMIT 1\"\n",
    "                    df = spark.sql(SQL_Query)\n",
    "                    num_cols_table = len(df.columns)                \n",
    "\n",
    "                    #count number of columns in json file                \n",
    "                    f = open(folder_path_json + filename)\n",
    "                    schema = json.load(f)\n",
    "                    has_attributes = schema[\"definitions\"][0][\"hasAttributes\"]  \n",
    "                    num_names = len(has_attributes)\n",
    "\n",
    "                    if num_cols_table != num_names:\n",
    "                        if is_string_empty(schema_name):\n",
    "                            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + table_name)\n",
    "                        else:\n",
    "                            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + schema_name +\".\" + table_name)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def is_string_empty(s):\n",
    "    return not bool(s.strip())\n",
    "\n",
    "if os.path.exists(folder_path_reset):\n",
    "    for filename in os.listdir(folder_path_reset):\n",
    "        # Remove the table\n",
    "        table_name = filename.replace(\"-\",\"\")\n",
    "        table_name = table_name.replace(\".txt\",\"\")\n",
    "\n",
    "        if is_string_empty(schema_name):\n",
    "            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + table_name)\n",
    "        else:\n",
    "            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + schema_name +\".\" + table_name) \n",
    "\n",
    "        try:  \n",
    "            os.remove(folder_path_reset + '/' + filename)  \n",
    "        except OSError as e:  # this would catch any error when trying to delete the file  \n",
    "            print(f\"Error: {filename} : {e.strerror}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594c099-6512-4777-82e2-9a3a058512fe",
   "metadata": {
    "cellStatus": "{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7249665Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:07.7601315Z\",\"execution_finish_time\":\"2023-08-15T09:15:18.128035Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "file_list = []\n",
    "\n",
    "def is_string_empty(s):\n",
    "    return not bool(s.strip())\n",
    "\n",
    "for entry in os.scandir(folder_path):\n",
    " if entry.is_dir():\n",
    "    # Get the list of all file paths in the directory  \n",
    "    files_in_dir = glob.glob(folder_path + entry.name + '/*')\n",
    "    if not files_in_dir:\n",
    "        continue\n",
    "    \n",
    "    table_name = entry.name.replace(\"-\",\"\")\n",
    "    ContainsCompany = False\n",
    "    df_new = spark.read.option(\"minPartitions\", no_Partition).format(\"csv\").option(\"header\",\"true\").load(folder_path_spark + entry.name +\"/*\")   \n",
    "\n",
    "    f = open(folder_path_json + entry.name +\".cdm.json\")\n",
    "    schema = json.load(f)\n",
    "    # Parse the schema to get column names and data types\n",
    "    column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n",
    "    if '$Company' in column_names:\n",
    "        ContainsCompany = True\n",
    "    column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n",
    "    for col_name, col_type in zip(column_names, column_types):\n",
    "        if col_type == \"String\":\n",
    "            col_type = \"string\"\n",
    "        if col_type == \"Guid\":\n",
    "            col_type = \"string\"\n",
    "        if col_type == \"Code\":\n",
    "            col_type = \"object\"\n",
    "        if col_type == \"Option\":\n",
    "            col_type = \"string\"\n",
    "        if col_type == \"Date\":\n",
    "            col_type = \"date\"\n",
    "        if col_type == \"Time\":\n",
    "            col_type = \"string\"\n",
    "        if col_type == \"DateTime\":\n",
    "            col_type = DateTimeFormat\n",
    "        if col_type == \"Duration\":\n",
    "            col_type = \"timedelta\"\n",
    "        if col_type == \"Decimal\":\n",
    "            col_type = DecimalFormat\n",
    "        if col_type == \"Boolean\":\n",
    "            col_type = \"boolean\"\n",
    "        if col_type == \"Integer\":\n",
    "            col_type = \"int\"\n",
    "        if col_type == \"Int64\":\n",
    "            col_type = \"int\"\n",
    "        if col_type == \"Int32\":\n",
    "            col_type = \"int\"\n",
    "        if col_name == 'SystemModifiedAt-2000000003': #Audit fields must be in timestamp\n",
    "            col_type = \"timestamp\"\n",
    "        if col_name == 'SystemCreatedAt-2000000001': \n",
    "            col_type = \"timestamp\"\n",
    "\n",
    "        df_new = df_new.withColumn(col_name, df_new[col_name].cast(col_type))\n",
    "\n",
    "    #check if the table exists\n",
    "    if table_name in [t.name for t in spark.catalog.listTables()]:\n",
    "        #read the old data into a new dataframe and union with the new dataframe\n",
    "        SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;  \n",
    "        #print(SQL_Query)\n",
    "        df_old = spark.sql(SQL_Query)\n",
    "        df_new = df_new.union(df_old).repartition(no_Partition)\n",
    "\n",
    "        #delete all old records\n",
    "        df_deletes = df_new.filter(df_new['SystemCreatedAt-2000000001'].isNull())\n",
    "        if ContainsCompany:\n",
    "            df_new = df_new.join(df_deletes, ['$Company','systemId-2000000000'], 'leftanti')\n",
    "        else:\n",
    "            df_new = df_new.join(df_deletes, ['systemId-2000000000'], 'leftanti')\n",
    "        \n",
    "        # remove duplicates by filtering on systemID and systemModifiedAt fields\n",
    "        if ContainsCompany:\n",
    "            df_new = df_new.orderBy('$Company','systemId-2000000000',desc('SystemModifiedAt-2000000003'))\n",
    "            df_new = df_new.dropDuplicates(['$Company','systemId-2000000000'])\n",
    "        else:\n",
    "            df_new = df_new.orderBy('systemId-2000000000',desc('SystemModifiedAt-2000000003'))\n",
    "            df_new = df_new.dropDuplicates(['systemId-2000000000'])\n",
    "        \n",
    "        #overwrite the dataframe in the new table\n",
    "        if is_string_empty(schema_name):\n",
    "            df_new.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{table_name}\")\n",
    "        else:\n",
    "            df_new.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{schema_name}.{table_name}\")\n",
    "    else:  \n",
    "        #table isn't there so just insert it\n",
    "        if is_string_empty(schema_name):\n",
    "            df_new.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{table_name}\")\n",
    "        else:\n",
    "            df_new.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{schema_name}.{table_name}\")        \n",
    "\n",
    "    #delete the files\n",
    "    if Remove_delta:\n",
    "        for filename in files_in_dir:\n",
    "            try:  \n",
    "                os.remove(filename)  \n",
    "            except OSError as e:  # this would catch any error when trying to delete the file  \n",
    "                print(f\"Error: {filename} : {e.strerror}\")\n",
    "        files_in_dir = [] # clear the list"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "synapse_widget": {
     "state": {},
     "token": "a69b4b72-86b0-4373-b695-ef01cd53bbb1"
    },
    "trident": {
     "lakehouse": {
      "default_lakehouse": "9fbacb3e-d0df-43a4-814b-abe4cb623a81",
      "known_lakehouses": "[{\"id\":\"9fbacb3e-d0df-43a4-814b-abe4cb623a81\"}]"
     }
    }
   },
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "9fbacb3e-d0df-43a4-814b-abe4cb623a81",
    "default_lakehouse_name": "businessCentral",
    "default_lakehouse_workspace_id": "21a92229-a0fb-4256-86bd-4b847b8006ed",
    "known_lakehouses": [
     {
      "id": "9fbacb3e-d0df-43a4-814b-abe4cb623a81"
     }
    ]
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
