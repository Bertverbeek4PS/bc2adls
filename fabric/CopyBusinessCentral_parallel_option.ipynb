{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae53e9bf-8787-4d07-b709-d896fd16cc5f",
   "metadata": {
    "editable": false,
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "source": [
    "## Business Central merge data notebook\n",
    "In this part the files in the delta folder will be merge with the Lakehouse table.\n",
    "- It iterates first on the folders to append to the existing table.\n",
    "- After that is will remove all duplicates by sorting the table. \n",
    "- At last it will remove all deleted records inside the table that are deleted in Business Central\n",
    "\n",
    "Please change the parameters in the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc5721-e317-4dc0-88ef-2c6bafb494da",
   "metadata": {
    "cellStatus": "{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.6812441Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:06.8530455Z\",\"execution_finish_time\":\"2023-08-15T09:15:07.1828235Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "# settings\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizewrite.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "spark.conf.set(\"spark.sql.delta.commitProtocol.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.analyzer.maxIterations\", \"999\")\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# file paths\n",
    "folder_path_spark = 'Files/deltas/' # this is mostly the default\n",
    "folder_path_json = '/lakehouse/default/Files/' # this is mostly the default\n",
    "folder_path_reset = '/lakehouse/default/Files/reset/' # this is mostly the default\n",
    "folder_path = '/lakehouse/default/Files/deltas/' # this is mostly the default\n",
    "\n",
    "# parameters\n",
    "workspace = 'businessCentral' #can also be a GUID\n",
    "Lakehouse = 'businessCentral'; #can also be a GUID - if you do please add back-quotes around the GUID for example: '`GUID`'\n",
    "Remove_delta = True; #will remove the delta files if everything is processed\n",
    "Drop_table_if_mismatch = False; #option to drop the table if json file has different columns then in the table\n",
    "no_Partition = 258 #how many partition is used in the dataframe, a good starting point might be 2-4 partitions per CPU core in your Spark cluster\n",
    "DecimalFormat = 'float' #how to format the decimal numbers, can be 'float' or 'decimal(10,3)'. If you change this it will be a breaking change for the table\n",
    "DateTimeFormat = 'timestamp' #how to format the datetime, can be 'timestamp' or 'date'. If you change this it will be a breaking change for the table\n",
    "schema_name = \"\" #for if you are using a lakehouse based on a schema\n",
    "max_parallel_tables = 4 #number of tables to process in parallel (adjust based on cluster size)\n",
    "use_delta_merge = False #use Delta MERGE operations instead of union+overwrite (EXPERIMENTAL - set to False if errors occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def is_string_empty(s):\n",
    "    return not bool(s.strip())\n",
    "\n",
    "if Drop_table_if_mismatch:\n",
    "\n",
    "    def count_keys(obj):  \n",
    "        if isinstance(obj, dict):  \n",
    "            return len(obj) + sum(count_keys(v) for v in obj.values())  \n",
    "        if isinstance(obj, list):  \n",
    "            return sum(count_keys(v) for v in obj)  \n",
    "        return 0  \n",
    "\n",
    "    for filename in os.listdir(folder_path_json):\n",
    "        if \"manifest\" not in filename: # exclude the manifest files\n",
    "            if filename.endswith(\".cdm.json\"):\n",
    "                table_name = filename.replace(\"-\",\"\")\n",
    "                table_name = table_name.replace(\".cdm.json\",\"\")\n",
    "\n",
    "                if table_name in [t.name for t in spark.catalog.listTables()]:\n",
    "                    #count number of columns in excisting table\n",
    "                    if is_string_empty(schema_name):\n",
    "                        SQL_Query = \"SELECT * FROM \" + Lakehouse + \".\" + table_name + \" LIMIT 1\"\n",
    "                    else:\n",
    "                        SQL_Query = \"SELECT * FROM \" + Lakehouse + \".\" + schema_name +\".\" + table_name + \" LIMIT 1\"\n",
    "                    df = spark.sql(SQL_Query)\n",
    "                    num_cols_table = len(df.columns)                \n",
    "\n",
    "                    #count number of columns in json file                \n",
    "                    f = open(folder_path_json + filename)\n",
    "                    schema = json.load(f)\n",
    "                    has_attributes = schema[\"definitions\"][0][\"hasAttributes\"]  \n",
    "                    num_names = len(has_attributes)\n",
    "\n",
    "                    if num_cols_table != num_names:\n",
    "                        if is_string_empty(schema_name):\n",
    "                            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + table_name)\n",
    "                        else:\n",
    "                            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + schema_name +\".\" + table_name)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def is_string_empty(s):\n",
    "    return not bool(s.strip())\n",
    "\n",
    "if os.path.exists(folder_path_reset):\n",
    "    for filename in os.listdir(folder_path_reset):\n",
    "        # Remove the table\n",
    "        table_name = filename.replace(\"-\",\"\")\n",
    "        table_name = table_name.replace(\".txt\",\"\")\n",
    "\n",
    "        if is_string_empty(schema_name):\n",
    "            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + table_name)\n",
    "        else:\n",
    "            df = spark.sql(\"DROP TABLE IF EXISTS \"+ Lakehouse + \".\" + schema_name +\".\" + table_name) \n",
    "\n",
    "        try:  \n",
    "            os.remove(folder_path_reset + '/' + filename)  \n",
    "        except OSError as e:  # this would catch any error when trying to delete the file  \n",
    "            print(f\"Error: {filename} : {e.strerror}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594c099-6512-4777-82e2-9a3a058512fe",
   "metadata": {
    "cellStatus": "{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7249665Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:07.7601315Z\",\"execution_finish_time\":\"2023-08-15T09:15:18.128035Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col, desc, expr\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "def is_string_empty(s):\n",
    "    return not bool(s.strip())\n",
    "\n",
    "def get_table_name(schema_name_val, lakehouse_val, table_val):\n",
    "    \"\"\"Helper to construct fully qualified table name\"\"\"\n",
    "    if is_string_empty(schema_name_val):\n",
    "        return f\"{lakehouse_val}.{table_val}\"\n",
    "    else:\n",
    "        return f\"{lakehouse_val}.{schema_name_val}.{table_val}\"\n",
    "\n",
    "def convert_datatype(col_type, col_name):\n",
    "    \"\"\"Convert CDM data types to Spark data types\"\"\"\n",
    "    type_mapping = {\n",
    "        \"String\": \"string\",\n",
    "        \"Guid\": \"string\",\n",
    "        \"Code\": \"string\",\n",
    "        \"Option\": \"string\",\n",
    "        \"Date\": \"date\",\n",
    "        \"Time\": \"string\",\n",
    "        \"DateTime\": DateTimeFormat,\n",
    "        \"Duration\": \"string\",\n",
    "        \"Decimal\": DecimalFormat,\n",
    "        \"Boolean\": \"boolean\",\n",
    "        \"Integer\": \"int\",\n",
    "        \"Int64\": \"int\",\n",
    "        \"Int32\": \"int\"\n",
    "    }\n",
    "    \n",
    "    # Special handling for audit fields\n",
    "    if col_name in ['SystemModifiedAt-2000000003', 'SystemCreatedAt-2000000001']:\n",
    "        return \"timestamp\"\n",
    "    \n",
    "    return type_mapping.get(col_type, \"string\")\n",
    "\n",
    "def process_table(entry_name, files_in_dir):\n",
    "    \"\"\"Process a single table - designed to run in parallel\"\"\"\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        table_name = entry_name.replace(\"-\",\"\")\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Processing table: {table_name}\")\n",
    "        \n",
    "        # Read new data with schema inference disabled for performance\n",
    "        df_new = spark.read.option(\"minPartitions\", no_Partition).format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"false\").load(folder_path_spark + entry_name +\"/*\")\n",
    "        \n",
    "        # Check if dataframe has data\n",
    "        if df_new.rdd.isEmpty():\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] ⚠ Skipping {table_name} - no data found\")\n",
    "            return {\"table\": table_name, \"status\": \"skipped\", \"reason\": \"no data\"}\n",
    "        \n",
    "        # Load and parse schema\n",
    "        with open(folder_path_json + entry_name +\".cdm.json\") as f:\n",
    "            schema = json.load(f)\n",
    "        \n",
    "        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n",
    "        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]\n",
    "        \n",
    "        ContainsCompany = '$Company' in column_names\n",
    "        \n",
    "        # Apply schema transformations\n",
    "        for col_name, col_type in zip(column_names, column_types):\n",
    "            spark_type = convert_datatype(col_type, col_name)\n",
    "            df_new = df_new.withColumn(col_name, col(col_name).cast(spark_type))\n",
    "        \n",
    "        # Create temp view for new data\n",
    "        temp_view = f\"temp_{table_name}_{os.getpid()}\"\n",
    "        df_new.createOrReplaceTempView(temp_view)\n",
    "        \n",
    "        full_table_name = get_table_name(schema_name, Lakehouse, table_name)\n",
    "        \n",
    "        # Check if table exists using try/except instead of catalog listing (thread-safe)\n",
    "        table_exists = False\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE TABLE {full_table_name}\").collect()\n",
    "            table_exists = True\n",
    "        except:\n",
    "            table_exists = False\n",
    "        \n",
    "        if table_exists and use_delta_merge:\n",
    "            # Use DELTA MERGE - much faster than union + overwrite\n",
    "            if ContainsCompany:\n",
    "                key_condition = \"target.`$Company` = source.`$Company` AND target.`systemId-2000000000` = source.`systemId-2000000000`\"\n",
    "            else:\n",
    "                key_condition = \"target.`systemId-2000000000` = source.`systemId-2000000000`\"\n",
    "            \n",
    "            # Build MERGE statement with proper formatting\n",
    "            merge_sql = f\"MERGE INTO {full_table_name} as target USING {temp_view} as source ON {key_condition} \"\n",
    "            merge_sql += \"WHEN MATCHED AND source.`SystemCreatedAt-2000000001` IS NULL THEN DELETE \"\n",
    "            merge_sql += \"WHEN MATCHED AND source.`SystemModifiedAt-2000000003` > target.`SystemModifiedAt-2000000003` THEN UPDATE SET * \"\n",
    "            merge_sql += \"WHEN NOT MATCHED AND source.`SystemCreatedAt-2000000001` IS NOT NULL THEN INSERT *\"\n",
    "            \n",
    "            try:\n",
    "                spark.sql(merge_sql)\n",
    "            except Exception as merge_error:\n",
    "                print(f\"MERGE SQL failed for {table_name}.\")\n",
    "                print(f\"Full table name: {full_table_name}\")\n",
    "                print(f\"Temp view: {temp_view}\")\n",
    "                print(f\"Key condition: {key_condition}\")\n",
    "                print(f\"SQL length: {len(merge_sql)}\")\n",
    "                print(f\"SQL: {merge_sql}\")\n",
    "                raise merge_error\n",
    "            \n",
    "        elif table_exists:\n",
    "            # Fallback to union approach if merge is disabled\n",
    "            df_old = spark.sql(f\"SELECT * FROM {full_table_name}\")\n",
    "            df_combined = df_new.union(df_old)\n",
    "            \n",
    "            # Filter out deletes\n",
    "            df_deletes = df_combined.filter(col('SystemCreatedAt-2000000001').isNull())\n",
    "            if ContainsCompany:\n",
    "                df_combined = df_combined.join(df_deletes, ['$Company','systemId-2000000000'], 'leftanti')\n",
    "            else:\n",
    "                df_combined = df_combined.join(df_deletes, ['systemId-2000000000'], 'leftanti')\n",
    "            \n",
    "            # Remove duplicates\n",
    "            if ContainsCompany:\n",
    "                df_combined = df_combined.orderBy('$Company','systemId-2000000000', desc('SystemModifiedAt-2000000003'))\n",
    "                df_combined = df_combined.dropDuplicates(['$Company','systemId-2000000000'])\n",
    "            else:\n",
    "                df_combined = df_combined.orderBy('systemId-2000000000', desc('SystemModifiedAt-2000000003'))\n",
    "                df_combined = df_combined.dropDuplicates(['systemId-2000000000'])\n",
    "            \n",
    "            # Coalesce instead of repartition for better performance\n",
    "            df_combined = df_combined.coalesce(no_Partition // 2)\n",
    "            df_combined.write.mode(\"overwrite\").format(\"delta\").saveAsTable(full_table_name)\n",
    "        else:\n",
    "            # New table - just write\n",
    "            df_new.coalesce(no_Partition // 4).write.mode(\"overwrite\").format(\"delta\").saveAsTable(full_table_name)\n",
    "        \n",
    "        # Clean up temp view\n",
    "        spark.catalog.dropTempView(temp_view)\n",
    "        \n",
    "        # Delete delta files if configured\n",
    "        if Remove_delta:\n",
    "            for filename in files_in_dir:\n",
    "                try:  \n",
    "                    os.remove(filename)  \n",
    "                except OSError as e:  \n",
    "                    print(f\"Error deleting {filename}: {e.strerror}\")\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] ✓ Completed {table_name} in {duration:.1f}s\")\n",
    "        return {\"table\": table_name, \"status\": \"success\", \"duration\": duration}\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] ✗ Error processing {table_name}: {str(e)}\")\n",
    "        print(f\"Full traceback:\\n{error_details}\")\n",
    "        return {\"table\": table_name, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Collect all tables to process\n",
    "tables_to_process = []\n",
    "for entry in os.scandir(folder_path):\n",
    "    if entry.is_dir():\n",
    "        files_in_dir = glob.glob(folder_path + entry.name + '/*')\n",
    "        if files_in_dir:\n",
    "            tables_to_process.append((entry.name, files_in_dir))\n",
    "\n",
    "print(f\"Found {len(tables_to_process)} tables to process\")\n",
    "print(f\"Processing with max {max_parallel_tables} tables in parallel\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process tables in parallel\n",
    "start_time = datetime.now()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_parallel_tables) as executor:\n",
    "    future_to_table = {executor.submit(process_table, entry_name, files): entry_name \n",
    "                       for entry_name, files in tables_to_process}\n",
    "    \n",
    "    for future in as_completed(future_to_table):\n",
    "        results.append(future.result())\n",
    "\n",
    "# Summary\n",
    "total_duration = (datetime.now() - start_time).total_seconds()\n",
    "success_count = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "error_count = len(results) - success_count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Processing completed in {total_duration:.1f}s\")\n",
    "print(f\"Success: {success_count} | Errors: {error_count}\")\n",
    "if error_count > 0:\n",
    "    print(\"\\nFailed tables:\")\n",
    "    for r in results:\n",
    "        if r[\"status\"] == \"error\":\n",
    "            print(f\"  - {r['table']}: {r['error']}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "synapse_widget": {
     "state": {},
     "token": "a69b4b72-86b0-4373-b695-ef01cd53bbb1"
    },
    "trident": {
     "lakehouse": {
      "default_lakehouse": "9fbacb3e-d0df-43a4-814b-abe4cb623a81",
      "known_lakehouses": "[{\"id\":\"9fbacb3e-d0df-43a4-814b-abe4cb623a81\"}]"
     }
    }
   },
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "9fbacb3e-d0df-43a4-814b-abe4cb623a81",
    "default_lakehouse_name": "businessCentral",
    "default_lakehouse_workspace_id": "21a92229-a0fb-4256-86bd-4b847b8006ed",
    "known_lakehouses": [
     {
      "id": "9fbacb3e-d0df-43a4-814b-abe4cb623a81"
     }
    ]
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
